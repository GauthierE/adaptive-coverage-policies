{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# transformations for the test dataset\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# load CIFAR-10 test dataset\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# define a function to unnormalize and convert the image back to a NumPy array\n",
    "def unnormalize_image(image_tensor):\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    image = image_tensor.numpy().transpose((1, 2, 0))  # convert from (C, H, W) to (H, W, C)\n",
    "    image = std * image + mean  # unnormalize\n",
    "    image = np.clip(image, 0, 1)  # clip to valid range [0, 1]\n",
    "    return image\n",
    "\n",
    "# plot image with its label distribution\n",
    "def plot_image(image_tensor, label):\n",
    "    # unnormalize and convert image to NumPy format\n",
    "    image = unnormalize_image(image_tensor)\n",
    "    \n",
    "    # plot image and label distribution\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    # image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[label])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# show examples for specific indices\n",
    "for i in range(2): \n",
    "    image_tensor, label = testset[i]\n",
    "    plot_image(image_tensor,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# model\n",
    "net = EfficientNetB0()\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "weights_path = \"weights/EfficientNetB0_0.1_100_512_SGD_1\" \n",
    "\n",
    "# load weights into the model\n",
    "net.load_state_dict(torch.load(weights_path))\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_size = 100\n",
    "total_test_size = len(testset)  # 10000\n",
    "eval_size = total_test_size-calibration_size\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# fix seed for reproducibility\n",
    "seed = 42\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "indices = torch.randperm(total_test_size, generator=generator)\n",
    "\n",
    "calibration_indices = indices[:calibration_size]\n",
    "eval_indices = indices[calibration_size:]\n",
    "\n",
    "calibration_subset = Subset(testset, calibration_indices)\n",
    "eval_subset = Subset(testset, eval_indices)\n",
    "\n",
    "print(len(calibration_subset))  \n",
    "print(len(eval_subset))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# configuration\n",
    "lambda_reg = 5\n",
    "k = 100 # sigmoid approx parameter\n",
    "lr = 1e-3\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "num_epochs = 2000 \n",
    "\n",
    "# neural network definition\n",
    "class AlphaNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "# smoothed size function (sigmoid approximation)\n",
    "def smooth_size(scores, alpha, T, n, k):\n",
    "    frac = (n + 1) * scores / (T + scores)\n",
    "    threshold = 1.0 / alpha\n",
    "    soft_indicators = torch.sigmoid(-k * (frac - threshold))\n",
    "    return soft_indicators.sum()\n",
    "\n",
    "input_dim = num_classes + 1 # +1 corresponds to the sum of calibration scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run this cell only if you want to rebuild a training leave-one-out dataset\n",
    "### otherwise just run the next cell to load one\n",
    "\n",
    "print(\"Building training dataset (leave-one-out calibration)...\")\n",
    "\n",
    "train_inputs = []\n",
    "train_sizes = []\n",
    "\n",
    "# loop over calibration subset\n",
    "for i in range(len(calibration_subset)):\n",
    "    print(i)\n",
    "    # define calibration set (all except i)\n",
    "    calibration_scores = []\n",
    "    with torch.no_grad():\n",
    "        for j in range(len(calibration_subset)):\n",
    "            if j == i:  # skip the test point\n",
    "                continue\n",
    "            x_calib, y_calib = calibration_subset[j]\n",
    "            x_calib = x_calib.unsqueeze(0).to(device)\n",
    "            y_calib = torch.tensor([y_calib], dtype=torch.long).to(device)\n",
    "            logits = net(x_calib)\n",
    "            score = criterion(logits, y_calib).item()\n",
    "            calibration_scores.append(score)\n",
    "\n",
    "    # compute sum of calibration scores\n",
    "    T = torch.tensor(sum(calibration_scores), dtype=torch.float32).to(device)\n",
    "\n",
    "    # take the i-th sample as the test point\n",
    "    x_test, _ = calibration_subset[i]\n",
    "    x_test = x_test.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = net(x_test).squeeze(0)  # (num_classes,)\n",
    "        scores = []\n",
    "        for cls in range(num_classes):\n",
    "            label_tensor = torch.tensor([cls], dtype=torch.long).to(device)\n",
    "            score_cls = criterion(logits.unsqueeze(0), label_tensor).item()\n",
    "            scores.append(score_cls)\n",
    "\n",
    "    # build feature vector\n",
    "    scores_tensor = torch.tensor(scores, dtype=torch.float32).to(device)\n",
    "    input_feat = torch.cat([scores_tensor, T.view(1)])  # (num_classes + 1,)\n",
    "\n",
    "    train_inputs.append(input_feat)\n",
    "    train_sizes.append(scores_tensor)\n",
    "\n",
    "print(\"Finished dataset generation.\")\n",
    "\n",
    "# convert to TensorDataset \n",
    "X_train = torch.stack(train_inputs)          # (N, num_classes + 1)\n",
    "S_train = torch.stack(train_sizes)           # (N, num_classes)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, S_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# save training dataset\n",
    "torch.save({\n",
    "    'X_train': X_train,\n",
    "    'S_train': S_train\n",
    "}, 'loo_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### uncomment below to load training leave-one-out dataset\n",
    "\n",
    "# data = torch.load('loo_data.pt')\n",
    "\n",
    "# X_train = data['X_train']\n",
    "# S_train = data['S_train']\n",
    "\n",
    "# train_dataset = TensorDataset(X_train, S_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training neural network\n",
    "\n",
    "all_losses = []\n",
    "all_sizes = []\n",
    "all_alphas = []\n",
    "\n",
    "for run in range(5): # average over 5 runs\n",
    "    print(f\"Run {run+1}/5\")\n",
    "    torch.manual_seed(run)\n",
    "    np.random.seed(run)\n",
    "\n",
    "    losses_per_epoch = []\n",
    "    sizes_per_epoch = []\n",
    "    alphas_per_epoch = []\n",
    "\n",
    "    alpha_net = AlphaNet(input_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(alpha_net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        alpha_net.train()\n",
    "        total_loss = 0\n",
    "        total_size = 0\n",
    "        total_alpha = 0\n",
    "\n",
    "        for x_batch, s_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            s_batch = s_batch.to(device)\n",
    "            T_batch = x_batch[:, -1]\n",
    "            alpha_batch = alpha_net(x_batch)\n",
    "\n",
    "            sizes = []\n",
    "            for i in range(x_batch.size(0)):\n",
    "                size_i = smooth_size(\n",
    "                    scores=s_batch[i],\n",
    "                    alpha=alpha_batch[i],\n",
    "                    T=T_batch[i],\n",
    "                    n=calibration_size-1,# /!\\ n-1 here\n",
    "                    k=k\n",
    "                )\n",
    "                sizes.append(size_i)\n",
    "\n",
    "            sizes = torch.stack(sizes)\n",
    "            losses = sizes + lambda_reg * alpha_batch\n",
    "            loss = losses.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_size += sizes.mean().item()\n",
    "            total_alpha += alpha_batch.mean().item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_size = total_size / len(train_loader)\n",
    "        avg_alpha = total_alpha / len(train_loader)\n",
    "\n",
    "        losses_per_epoch.append(avg_loss)\n",
    "        sizes_per_epoch.append(avg_size)\n",
    "        alphas_per_epoch.append(avg_alpha)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | \"\n",
    "              f\"Mean Size: {avg_size:.2f} | Mean Alpha: {avg_alpha:.4f}\")\n",
    "\n",
    "    all_losses.append(losses_per_epoch)\n",
    "    all_sizes.append(sizes_per_epoch)\n",
    "    all_alphas.append(alphas_per_epoch)\n",
    "\n",
    "# torch.save(alpha_net.state_dict(), f\"output/alpha_net_{lambda_reg}.pth\")\n",
    "\n",
    "# np.save(f\"output/all_losses_{lambda_reg}.npy\", np.array(all_losses))\n",
    "# np.save(f\"output/all_sizes_{lambda_reg}.npy\", np.array(all_sizes))\n",
    "# np.save(f\"output/all_alphas_{lambda_reg}.npy\", np.array(all_alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from tueplots import bundles\n",
    "\n",
    "all_losses_5 = np.load(\"output/all_losses_5.npy\")\n",
    "all_sizes_5 = np.load(\"output/all_sizes_5.npy\")\n",
    "all_alphas_5 = np.load(\"output/all_alphas_5.npy\")\n",
    "\n",
    "all_losses_10 = np.load(\"output/all_losses_10.npy\")\n",
    "all_sizes_10 = np.load(\"output/all_sizes_10.npy\")\n",
    "all_alphas_10 = np.load(\"output/all_alphas_10.npy\")\n",
    "\n",
    "all_losses_50 = np.load(\"output/all_losses_50.npy\")\n",
    "all_sizes_50 = np.load(\"output/all_sizes_50.npy\")\n",
    "all_alphas_50 = np.load(\"output/all_alphas_50.npy\")\n",
    "\n",
    "# convert to arrays (usually already arrays, but safe)\n",
    "all_losses_5 = np.array(all_losses_5)\n",
    "all_losses_10 = np.array(all_losses_10)\n",
    "all_losses_50 = np.array(all_losses_50)\n",
    "\n",
    "all_sizes_5 = np.array(all_sizes_5)\n",
    "all_sizes_10 = np.array(all_sizes_10)\n",
    "all_sizes_50 = np.array(all_sizes_50)\n",
    "\n",
    "all_alphas_5 = np.array(all_alphas_5)\n",
    "all_alphas_10 = np.array(all_alphas_10)\n",
    "all_alphas_50 = np.array(all_alphas_50)\n",
    "\n",
    "# define moving average smoothing function\n",
    "def moving_average(arr, window_size):\n",
    "    return np.convolve(arr, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def smooth_all(arr_2d, window):\n",
    "    smoothed_runs = []\n",
    "    for run in arr_2d:\n",
    "        smoothed = moving_average(run, window)\n",
    "        smoothed_runs.append(smoothed)\n",
    "    smoothed_runs = np.array(smoothed_runs)\n",
    "    mean = np.nanmean(smoothed_runs, axis=0)\n",
    "    std = np.nanstd(smoothed_runs, axis=0)\n",
    "    return mean, std\n",
    "\n",
    "window_size = 50\n",
    "\n",
    "# compute smoothed means and stds\n",
    "mean_loss_5, std_loss_5 = smooth_all(all_losses_5, window_size)\n",
    "mean_loss_10, std_loss_10 = smooth_all(all_losses_10, window_size)\n",
    "mean_loss_50, std_loss_50 = smooth_all(all_losses_50, window_size)\n",
    "\n",
    "mean_size_5, std_size_5 = smooth_all(all_sizes_5, window_size)\n",
    "mean_size_10, std_size_10 = smooth_all(all_sizes_10, window_size)\n",
    "mean_size_50, std_size_50 = smooth_all(all_sizes_50, window_size)\n",
    "\n",
    "mean_alpha_5, std_alpha_5 = smooth_all(all_alphas_5, window_size)\n",
    "mean_alpha_10, std_alpha_10 = smooth_all(all_alphas_10, window_size)\n",
    "mean_alpha_50, std_alpha_50 = smooth_all(all_alphas_50, window_size)\n",
    "\n",
    "# adjust epochs due to smoothing window\n",
    "epochs = np.arange(1, all_losses_5.shape[1] + 1)\n",
    "epochs_smoothed = epochs[:len(mean_loss_5)]  # shortened by window_size - 1\n",
    "\n",
    "# style setup\n",
    "plt.rcParams.update(bundles.icml2024())\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 16,\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"axes.linewidth\": 2,\n",
    "})\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# colors for lambda = 5, 10, 50\n",
    "color_5 = \"#0077bb\"\n",
    "color_10 = \"#cc3311\"\n",
    "color_50 = \"#44aa99\" \n",
    "\n",
    "# plot 1: loss\n",
    "axs[0].plot(epochs_smoothed, mean_loss_5, label=r'$\\lambda=5$', color=color_5)\n",
    "axs[0].fill_between(epochs_smoothed,\n",
    "                    mean_loss_5 - std_loss_5,\n",
    "                    mean_loss_5 + std_loss_5,\n",
    "                    color=color_5, alpha=0.3)\n",
    "\n",
    "axs[0].plot(epochs_smoothed, mean_loss_10, label=r'$\\lambda=10$', color=color_10)\n",
    "axs[0].fill_between(epochs_smoothed,\n",
    "                    mean_loss_10 - std_loss_10,\n",
    "                    mean_loss_10 + std_loss_10,\n",
    "                    color=color_10, alpha=0.3)\n",
    "\n",
    "axs[0].plot(epochs_smoothed, mean_loss_50, label=r'$\\lambda=50$', color=color_50)\n",
    "axs[0].fill_between(epochs_smoothed,\n",
    "                    mean_loss_50 - std_loss_50,\n",
    "                    mean_loss_50 + std_loss_50,\n",
    "                    color=color_50, alpha=0.3)\n",
    "\n",
    "axs[0].set_xlim(-100, 2100)\n",
    "axs[0].set_ylim(0, 8)\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].set_title(\"Training Loss\")\n",
    "axs[0].grid(True)\n",
    "\n",
    "# plot 2: mean size\n",
    "axs[1].plot(epochs_smoothed, mean_size_5, label=r'$\\lambda=5$', color=color_5)\n",
    "axs[1].fill_between(epochs_smoothed,\n",
    "                    mean_size_5 - std_size_5,\n",
    "                    mean_size_5 + std_size_5,\n",
    "                    color=color_5, alpha=0.3)\n",
    "\n",
    "axs[1].plot(epochs_smoothed, mean_size_10, label=r'$\\lambda=10$', color=color_10)\n",
    "axs[1].fill_between(epochs_smoothed,\n",
    "                    mean_size_10 - std_size_10,\n",
    "                    mean_size_10 + std_size_10,\n",
    "                    color=color_10, alpha=0.3)\n",
    "\n",
    "axs[1].plot(epochs_smoothed, mean_size_50, label=r'$\\lambda=50$', color=color_50)\n",
    "axs[1].fill_between(epochs_smoothed,\n",
    "                    mean_size_50 - std_size_50,\n",
    "                    mean_size_50 + std_size_50,\n",
    "                    color=color_50, alpha=0.3)\n",
    "\n",
    "axs[1].set_xlim(-100, 2100)\n",
    "axs[1].set_ylim(0, 5)\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Mean Size\")\n",
    "axs[1].set_title(\"Mean Size per Epoch\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "# plot 3: mean miscoverage\n",
    "axs[2].plot(epochs_smoothed, mean_alpha_5, label=r'$\\lambda=5$', color=color_5)\n",
    "axs[2].fill_between(epochs_smoothed,\n",
    "                    mean_alpha_5 - std_alpha_5,\n",
    "                    mean_alpha_5 + std_alpha_5,\n",
    "                    color=color_5, alpha=0.3)\n",
    "\n",
    "axs[2].plot(epochs_smoothed, mean_alpha_10, label=r'$\\lambda=10$', color=color_10)\n",
    "axs[2].fill_between(epochs_smoothed,\n",
    "                    mean_alpha_10 - std_alpha_10,\n",
    "                    mean_alpha_10 + std_alpha_10,\n",
    "                    color=color_10, alpha=0.3)\n",
    "\n",
    "axs[2].plot(epochs_smoothed, mean_alpha_50, label=r'$\\lambda=50$', color=color_50)\n",
    "axs[2].fill_between(epochs_smoothed,\n",
    "                    mean_alpha_50 - std_alpha_50,\n",
    "                    mean_alpha_50 + std_alpha_50,\n",
    "                    color=color_50, alpha=0.3)\n",
    "\n",
    "axs[2].set_xlim(-100, 2100)\n",
    "axs[2].set_ylim(0, 0.25)\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].set_ylabel(r\"Mean $\\tilde\\alpha$\")\n",
    "axs[2].set_title(r\"Mean $\\tilde\\alpha$ per Epoch\")\n",
    "axs[2].grid(True)\n",
    "axs[2].legend(frameon=True)\n",
    "\n",
    "# minor ticks and formatting\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(5))  \n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator(2))  \n",
    "\n",
    "    ax.tick_params(which='both', length=4)\n",
    "    ax.tick_params(which='minor', length=2, width=1.5)\n",
    "    ax.tick_params(which='major', width=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/training.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_net = AlphaNet(input_dim).to(device)\n",
    "alpha_net.load_state_dict(torch.load(\"output/alpha_net_50.pth\", map_location=device))\n",
    "alpha_net.eval()\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "num_trials = 100\n",
    "coverages_adaptive, sizes_adaptive, alphas_adaptive = [], [], []\n",
    "coverages_fixed, sizes_fixed = [], []\n",
    "coverages_standard, sizes_standard = [], []\n",
    "\n",
    "# compute calibration sum\n",
    "cal_scores = []\n",
    "with torch.no_grad():\n",
    "    for x_c, y_c in calibration_subset:\n",
    "        x_c = x_c.unsqueeze(0).to(device)\n",
    "        y_c = torch.tensor([y_c], dtype=torch.long).to(device)\n",
    "        logit = net(x_c)\n",
    "        cal_scores.append(criterion(logit, y_c).item())\n",
    "T = torch.tensor(sum(cal_scores)).to(device)\n",
    "\n",
    "# helper: construct conformal set\n",
    "def conformal_set(scores, T, alpha):\n",
    "    frac = (calibration_size + 1) * scores / (T + scores)\n",
    "    return [y for y in range(num_classes) if frac[y] <= 1/alpha]\n",
    "\n",
    "# storage for reuse\n",
    "all_scores, all_y_test = [], []\n",
    "\n",
    "# e-adaptive evaluation\n",
    "for trial in range(num_trials):\n",
    "    # sample test point from eval_subset\n",
    "    idx = np.random.choice(len(eval_subset))\n",
    "    x_test, y_test = eval_subset[idx]\n",
    "    x_test = x_test.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = net(x_test).squeeze(0)\n",
    "\n",
    "    # compute per-class scores\n",
    "    scores = []\n",
    "    for y in range(num_classes):\n",
    "        y_tensor = torch.tensor([y], dtype=torch.long).to(device)\n",
    "        score = criterion(logits.unsqueeze(0), y_tensor).item()\n",
    "        scores.append(score)\n",
    "    scores_tensor = torch.tensor(scores).to(device)\n",
    "\n",
    "    # predict miscoverage\n",
    "    input_feat = torch.cat([scores_tensor, T.view(1)])\n",
    "    alpha_adapt = alpha_net(input_feat.unsqueeze(0)).item()\n",
    "\n",
    "    # conformal set\n",
    "    C_adapt = conformal_set(scores_tensor, T, alpha_adapt)\n",
    "\n",
    "    coverages_adaptive.append(int(y_test in C_adapt))\n",
    "    sizes_adaptive.append(len(C_adapt))\n",
    "    alphas_adaptive.append(alpha_adapt)\n",
    "\n",
    "    # store for reuse\n",
    "    all_scores.append(scores_tensor)\n",
    "    all_y_test.append(y_test)\n",
    "\n",
    "# e-fixed\n",
    "fixed_alpha = np.mean(alphas_adaptive)\n",
    "\n",
    "for i in range(num_trials):\n",
    "    scores_tensor = all_scores[i]\n",
    "    y_test = all_y_test[i]\n",
    "\n",
    "    C_fixed = conformal_set(scores_tensor, T, fixed_alpha)\n",
    "    coverages_fixed.append(int(y_test in C_fixed))\n",
    "    sizes_fixed.append(len(C_fixed))\n",
    "\n",
    "# p-fixed\n",
    "alpha = fixed_alpha  # use same alpha\n",
    "quantile_index = int(np.ceil((1 - alpha) * (calibration_size + 1))) - 1\n",
    "sorted_cal_scores = sorted(cal_scores)\n",
    "threshold = sorted_cal_scores[quantile_index]\n",
    "\n",
    "for i in range(num_trials):\n",
    "    scores_tensor = all_scores[i]\n",
    "    y_test = all_y_test[i]\n",
    "\n",
    "    C_standard = [y for y in range(num_classes) if scores_tensor[y].item() <= threshold]\n",
    "    coverages_standard.append(int(y_test in C_standard))\n",
    "    sizes_standard.append(len(C_standard))\n",
    "\n",
    "# results\n",
    "print(\"=== e-adaptive ===\")\n",
    "print(f\"Mean Alpha: {np.mean(alphas_adaptive):.4f}\")\n",
    "print(f\"Empirical Coverage: {np.mean(coverages_adaptive):.4f}\")\n",
    "print(f\"Expected Guarantee: {1 - np.mean(alphas_adaptive):.4f}\")\n",
    "print(f\"Average Set Size: {np.mean(sizes_adaptive):.2f}\")\n",
    "\n",
    "print(\"=== e-fixed ===\")\n",
    "print(f\"Fixed Alpha (mean adaptive): {fixed_alpha:.4f}\")\n",
    "print(f\"Empirical Coverage: {np.mean(coverages_fixed):.4f}\")\n",
    "print(f\"Expected Guarantee: {1 - fixed_alpha:.4f}\")\n",
    "print(f\"Average Set Size: {np.mean(sizes_fixed):.2f}\")\n",
    "\n",
    "print(\"=== p-fixed ===\")\n",
    "print(f\"Fixed Alpha (mean adaptive): {fixed_alpha:.4f}\")\n",
    "print(f\"Empirical Coverage: {np.mean(coverages_standard):.4f}\")\n",
    "print(f\"Expected Guarantee: {1 - fixed_alpha:.4f}\")\n",
    "print(f\"Average Set Size: {np.mean(sizes_standard):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tueplots import bundles\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "\n",
    "# style setup\n",
    "plt.rcParams.update(bundles.icml2024())\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 16,\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"axes.linewidth\": 2,\n",
    "})\n",
    "\n",
    "# plot histogram of alpha values\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "counts, bins, patches = ax.hist(np.array(alphas_adaptive),bins=15, color=\"#1f77b4\",\n",
    "                                 edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(r\"$\\tilde\\alpha$\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "ax.tick_params(which='both', length=4)\n",
    "ax.tick_params(which='minor', length=2, width=1.5)\n",
    "ax.tick_params(which='major', width=2)\n",
    "\n",
    "ax.set_xlim(0.03,0.082)\n",
    "ax.set_ylim(0,23)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/hist_alpha.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tueplots import bundles\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "# style setup\n",
    "plt.rcParams.update(bundles.icml2024())\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"axes.linewidth\": 2,\n",
    "})\n",
    "\n",
    "# helper to count occurrences for integer sizes 1,...,10\n",
    "def counts_for_sizes(sizes, min_size=1, max_size=10):\n",
    "    arr = np.asarray(sizes).astype(int)\n",
    "    arr = arr[(arr >= min_size) & (arr <= max_size)]\n",
    "    counts = np.bincount(arr, minlength=max_size + 1)\n",
    "    return counts[min_size:max_size + 1]\n",
    "\n",
    "# discrete x ticks (set sizes)\n",
    "x = np.arange(1, 11)        # 1,...,10\n",
    "width = 0.28                # bar width; middle bar sits on the tick\n",
    "\n",
    "# counts per size\n",
    "counts_adaptive = counts_for_sizes(sizes_adaptive)\n",
    "counts_fixed    = counts_for_sizes(sizes_fixed)\n",
    "counts_standard = counts_for_sizes(sizes_standard)\n",
    "\n",
    "# colors\n",
    "c_adapt    = \"#1f77b4\"  # blue\n",
    "c_fixed    = \"#ff7f0e\"  # orange\n",
    "c_standard = \"#2ca02c\"  # green\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "# 3 bars per tick: left (e-adaptive), middle (e-fixed), right (p-fixed)\n",
    "ax.bar(x - width, counts_adaptive, width=width, color=c_adapt,\n",
    "       edgecolor=\"black\", alpha=0.85, label=\"e-adaptive\")\n",
    "ax.bar(x,         counts_fixed,    width=width, color=c_fixed,\n",
    "       edgecolor=\"black\", alpha=0.85, label=\"e-fixed\")    # centered\n",
    "ax.bar(x + width, counts_standard, width=width, color=c_standard,\n",
    "       edgecolor=\"black\", alpha=0.85, label=\"p-fixed\")\n",
    "\n",
    "# vertical mean lines (matching colors)\n",
    "mean_adaptive = np.mean(sizes_adaptive)\n",
    "mean_fixed    = np.mean(sizes_fixed)\n",
    "mean_standard = np.mean(sizes_standard)\n",
    "\n",
    "ax.axvline(mean_adaptive, color=c_adapt, linestyle=\"--\", linewidth=2,\n",
    "           label=fr\"Mean e-adaptive = {mean_adaptive:.2f}\")\n",
    "ax.axvline(mean_fixed,    color=c_fixed, linestyle=\"--\", linewidth=2,\n",
    "           label=fr\"Mean e-fixed = {mean_fixed:.2f}\")\n",
    "ax.axvline(mean_standard, color=c_standard, linestyle=\"--\", linewidth=2,\n",
    "           label=fr\"Mean p-fixed = {mean_standard:.2f}\")\n",
    "\n",
    "# labels & ticks\n",
    "ax.set_xlabel(\"Set Size\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xticks(x)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "ax.tick_params(which='both', length=4)\n",
    "ax.tick_params(which='minor', length=2, width=1.5)\n",
    "ax.tick_params(which='major', width=2)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "order = [3, 4, 5, 0, 1, 2] \n",
    "ax.legend([handles[i] for i in order], [labels[i] for i in order], frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/hist_set_sizes.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
