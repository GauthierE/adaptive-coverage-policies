{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# transformations for the test dataset\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# load CIFAR-10 test dataset\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# define a function to unnormalize and convert the image back to a NumPy array\n",
    "def unnormalize_image(image_tensor):\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    image = image_tensor.numpy().transpose((1, 2, 0))  # convert from (C, H, W) to (H, W, C)\n",
    "    image = std * image + mean  # unnormalize\n",
    "    image = np.clip(image, 0, 1)  # clip to valid range [0, 1]\n",
    "    return image\n",
    "\n",
    "# plot image with its label distribution\n",
    "def plot_image(image_tensor, label):\n",
    "    # unnormalize and convert image to NumPy format\n",
    "    image = unnormalize_image(image_tensor)\n",
    "    \n",
    "    # plot image and label distribution\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    # image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[label])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# show examples for specific indices\n",
    "for i in range(2): \n",
    "    image_tensor, label = testset[i]\n",
    "    plot_image(image_tensor,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# model\n",
    "net = EfficientNetB0()\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "weights_path = \"weights/EfficientNetB0_0.1_100_512_SGD_1\" \n",
    "\n",
    "# load weights into the model\n",
    "net.load_state_dict(torch.load(weights_path))\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_size = 100\n",
    "total_test_size = len(testset)  # 10000\n",
    "eval_size = total_test_size-calibration_size\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# fix seed for reproducibility\n",
    "seed = 42\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "indices = torch.randperm(total_test_size, generator=generator)\n",
    "\n",
    "calibration_indices = indices[:calibration_size]\n",
    "eval_indices = indices[calibration_size:]\n",
    "\n",
    "calibration_subset = Subset(testset, calibration_indices)\n",
    "eval_subset = Subset(testset, eval_indices)\n",
    "\n",
    "print(len(calibration_subset))  \n",
    "print(len(eval_subset))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# configuration\n",
    "k = 100 # sigmoid approx parameter\n",
    "lr = 1e-3\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "num_epochs = 2000 \n",
    "\n",
    "# neural network definition\n",
    "class AlphaNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "# smoothed size function (sigmoid approximation)\n",
    "def smooth_size(scores, alpha, T, n, k):\n",
    "    frac = (n + 1) * scores / (T + scores)\n",
    "    threshold = 1.0 / alpha\n",
    "    soft_indicators = torch.sigmoid(-k * (frac - threshold))\n",
    "    return soft_indicators.sum()\n",
    "\n",
    "input_dim = num_classes + 1 # +1 corresponds to the sum of calibration scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run this cell only if you want to rebuild a training leave-one-out dataset\n",
    "### otherwise just run the next cell to load one\n",
    "\n",
    "print(\"Building training dataset (leave-one-out calibration)...\")\n",
    "\n",
    "train_inputs = []\n",
    "train_sizes = []\n",
    "\n",
    "# loop over calibration subset\n",
    "for i in range(len(calibration_subset)):\n",
    "    print(i)\n",
    "    # define calibration set (all except i)\n",
    "    calibration_scores = []\n",
    "    with torch.no_grad():\n",
    "        for j in range(len(calibration_subset)):\n",
    "            if j == i:  # skip the test point\n",
    "                continue\n",
    "            x_calib, y_calib = calibration_subset[j]\n",
    "            x_calib = x_calib.unsqueeze(0).to(device)\n",
    "            y_calib = torch.tensor([y_calib], dtype=torch.long).to(device)\n",
    "            logits = net(x_calib)\n",
    "            score = criterion(logits, y_calib).item()\n",
    "            calibration_scores.append(score)\n",
    "\n",
    "    # compute sum of calibration scores\n",
    "    T = torch.tensor(sum(calibration_scores), dtype=torch.float32).to(device)\n",
    "\n",
    "    # take the i-th sample as the test point\n",
    "    x_test, _ = calibration_subset[i]\n",
    "    x_test = x_test.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = net(x_test).squeeze(0)  # (num_classes,)\n",
    "        scores = []\n",
    "        for cls in range(num_classes):\n",
    "            label_tensor = torch.tensor([cls], dtype=torch.long).to(device)\n",
    "            score_cls = criterion(logits.unsqueeze(0), label_tensor).item()\n",
    "            scores.append(score_cls)\n",
    "\n",
    "    # build feature vector\n",
    "    scores_tensor = torch.tensor(scores, dtype=torch.float32).to(device)\n",
    "    input_feat = torch.cat([scores_tensor, T.view(1)])  # (num_classes + 1,)\n",
    "\n",
    "    train_inputs.append(input_feat)\n",
    "    train_sizes.append(scores_tensor)\n",
    "\n",
    "print(\"Finished dataset generation.\")\n",
    "\n",
    "# convert to TensorDataset \n",
    "X_train = torch.stack(train_inputs)          # (N, num_classes + 1)\n",
    "S_train = torch.stack(train_sizes)           # (N, num_classes)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, S_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# save training dataset\n",
    "torch.save({\n",
    "    'X_train': X_train,\n",
    "    'S_train': S_train\n",
    "}, 'loo_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### uncomment below to load training leave-one-out dataset\n",
    "\n",
    "# data = torch.load('loo_data.pt')\n",
    "\n",
    "# X_train = data['X_train']\n",
    "# S_train = data['S_train']\n",
    "\n",
    "# train_dataset = TensorDataset(X_train, S_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# parameters for lambda-selection\n",
    "M = 2           # target mean size\n",
    "epsilon = 0.1   # tolerance for bisection\n",
    "max_iters = 20  # max iterations for bracketing/bisection to avoid infinite loops\n",
    "\n",
    "# set seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# lists to track values\n",
    "lambda_history = []\n",
    "mean_sizes_history = []\n",
    "batch_sizes_history = []  # list of lists; each element is batch sizes for that lambda\n",
    "\n",
    "# function to train alpha_net for one lambda and return batch averages & mean size\n",
    "def train_and_compute_mean_size(lambda_reg, num_epochs=num_epochs):\n",
    "    alpha_net = AlphaNet(input_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(alpha_net.parameters(), lr=lr)\n",
    "\n",
    "    # to store average sizes per batch during training\n",
    "    avg_sizes_per_batch = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        alpha_net.train()\n",
    "        for x_batch, s_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            s_batch = s_batch.to(device)\n",
    "            T_batch = x_batch[:, -1]\n",
    "            alpha_batch = alpha_net(x_batch)\n",
    "\n",
    "            sizes = []\n",
    "            for i in range(x_batch.size(0)):\n",
    "                size_i = smooth_size(\n",
    "                    scores=s_batch[i],\n",
    "                    alpha=alpha_batch[i],\n",
    "                    T=T_batch[i],\n",
    "                    n=calibration_size-1,\n",
    "                    k=k\n",
    "                )\n",
    "                sizes.append(size_i)\n",
    "\n",
    "            sizes = torch.stack(sizes)\n",
    "            avg_sizes_per_batch.append(sizes.mean().item())  # track per batch\n",
    "\n",
    "            # loss and backprop\n",
    "            loss = (sizes + lambda_reg * alpha_batch).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # compute mean size over calibration set\n",
    "    alpha_net.eval()\n",
    "    all_sizes = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, s_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            s_batch = s_batch.to(device)\n",
    "            T_batch = x_batch[:, -1]\n",
    "            alpha_batch = alpha_net(x_batch)\n",
    "\n",
    "            sizes = []\n",
    "            for i in range(x_batch.size(0)):\n",
    "                size_i = smooth_size(\n",
    "                    scores=s_batch[i],\n",
    "                    alpha=alpha_batch[i],\n",
    "                    T=T_batch[i],\n",
    "                    n=calibration_size-1,\n",
    "                    k=k\n",
    "                )\n",
    "                sizes.append(size_i)\n",
    "\n",
    "            sizes = torch.stack(sizes)\n",
    "            all_sizes.append(sizes.mean().item())\n",
    "\n",
    "    mean_size = np.mean(all_sizes)\n",
    "\n",
    "    return alpha_net, mean_size, avg_sizes_per_batch\n",
    "\n",
    "\n",
    "# step 1: Expansion phase to bracket M\n",
    "lambda_reg = 40  # initial guess\n",
    "iter_count = 0\n",
    "lambda_low, lambda_high = None, None\n",
    "\n",
    "while iter_count < max_iters:\n",
    "    iter_count += 1\n",
    "    alpha_net, mean_size, avg_sizes_per_batch = train_and_compute_mean_size(lambda_reg)\n",
    "\n",
    "    # track values\n",
    "    lambda_history.append(lambda_reg)\n",
    "    mean_sizes_history.append(mean_size)\n",
    "    batch_sizes_history.append(avg_sizes_per_batch)\n",
    "\n",
    "    print(f\"Iteration {iter_count}: λ={lambda_reg:.5f}, mean_size={mean_size:.5f}\")\n",
    "\n",
    "    if mean_size < M:\n",
    "        lambda_low = lambda_reg  # last lambda that is below M\n",
    "        if lambda_high is not None:  # if we already have an upper bound, we are done\n",
    "            break\n",
    "        lambda_reg *= 2\n",
    "    else:\n",
    "        lambda_high = lambda_reg  # last lambda that is above M\n",
    "        if lambda_low is not None:  # if we already have a lower bound, we are done\n",
    "            break\n",
    "        lambda_reg /= 2\n",
    "\n",
    "print(f\"Bracket found: λ_low={lambda_low}, λ_high={lambda_high}, mean_size={mean_size:.5f}\")\n",
    "\n",
    "\n",
    "# step 2: Bisection refinement\n",
    "iter_count = 0\n",
    "while iter_count < max_iters:\n",
    "    iter_count += 1\n",
    "    lambda_reg = (lambda_low + lambda_high) / 2\n",
    "    alpha_net, mean_size, avg_sizes_per_batch = train_and_compute_mean_size(lambda_reg)\n",
    "\n",
    "    # track values\n",
    "    lambda_history.append(lambda_reg)\n",
    "    mean_sizes_history.append(mean_size)\n",
    "    batch_sizes_history.append(avg_sizes_per_batch)\n",
    "\n",
    "    print(f\"Iteration {iter_count}: λ={lambda_reg:.5f}, mean_size={mean_size:.5f}\")\n",
    "\n",
    "    if abs(mean_size - M) <= epsilon:\n",
    "        break\n",
    "    elif mean_size < M:\n",
    "        lambda_low = lambda_reg\n",
    "    else:\n",
    "        lambda_high = lambda_reg\n",
    "\n",
    "print(f\"Selected λ: {lambda_reg:.5f}, mean_size={mean_size:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_filename = f\"output/alpha_net_bracket_bisect.pth\"\n",
    "torch.save(alpha_net.state_dict(), final_filename)\n",
    "print(f\"Saved final model weights -> {final_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history\n",
    "history = {\n",
    "    \"lambda_history\": np.array(lambda_history),\n",
    "    \"mean_sizes_history\": np.array(mean_sizes_history),\n",
    "    \"batch_sizes_history\": batch_sizes_history,  # list of lists\n",
    "    \"M\": M,\n",
    "    \"epsilon\": epsilon\n",
    "}\n",
    "\n",
    "# save with numpy (binary .npz file)\n",
    "np.savez(\"output/lambda_selection_history.npz\", **history)\n",
    "print(\"History saved: lambda_selection_history.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .npz file\n",
    "data = np.load(\"output/lambda_selection_history.npz\", allow_pickle=True)\n",
    "\n",
    "lambda_history = data[\"lambda_history\"]\n",
    "mean_sizes_history = data[\"mean_sizes_history\"]\n",
    "batch_sizes_history = data[\"batch_sizes_history\"]  # stored as object array\n",
    "M = data[\"M\"].item()\n",
    "epsilon = data[\"epsilon\"].item()\n",
    "\n",
    "print(\"λ history:\", lambda_history)\n",
    "print(\"Mean sizes:\", mean_sizes_history)\n",
    "print(\"Batch sizes first run:\", batch_sizes_history[0])\n",
    "print(\"Target M:\", M, \"Tolerance ε:\", epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 100\n",
    "coverages_adaptive, sizes_adaptive, alphas_adaptive = [], [], []\n",
    "coverages_fixed, sizes_fixed = [], []\n",
    "coverages_standard, sizes_standard = [], []\n",
    "\n",
    "# set seed \n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# compute calibration scores once\n",
    "cal_scores = []\n",
    "with torch.no_grad():\n",
    "    for x_c, y_c in calibration_subset:\n",
    "        x_c = x_c.unsqueeze(0).to(device)\n",
    "        y_c = torch.tensor([y_c], dtype=torch.long).to(device)\n",
    "        logit = net(x_c)\n",
    "        cal_scores.append(criterion(logit, y_c).item())\n",
    "T = torch.tensor(sum(cal_scores)).to(device)\n",
    "\n",
    "# helper: construct conformal set\n",
    "def conformal_set(scores, T, alpha):\n",
    "    frac = (calibration_size + 1) * scores / (T + scores)\n",
    "    return [y for y in range(num_classes) if frac[y] <= 1/alpha]\n",
    "\n",
    "# storage for reuse\n",
    "all_scores, all_y_test = [], []\n",
    "\n",
    "# e-adaptive evaluation\n",
    "for trial in range(num_trials):\n",
    "    # sample test point from eval_subset\n",
    "    idx = np.random.choice(len(eval_subset))\n",
    "    x_test, y_test = eval_subset[idx]\n",
    "    x_test = x_test.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = net(x_test).squeeze(0)\n",
    "\n",
    "    # compute per-class scores\n",
    "    scores = []\n",
    "    for y in range(num_classes):\n",
    "        y_tensor = torch.tensor([y], dtype=torch.long).to(device)\n",
    "        score = criterion(logits.unsqueeze(0), y_tensor).item()\n",
    "        scores.append(score)\n",
    "    scores_tensor = torch.tensor(scores).to(device)\n",
    "\n",
    "    # predict miscoverage\n",
    "    input_feat = torch.cat([scores_tensor, T.view(1)])\n",
    "    alpha_adapt = alpha_net(input_feat.unsqueeze(0)).item()\n",
    "\n",
    "    # conformal set\n",
    "    C_adapt = conformal_set(scores_tensor, T, alpha_adapt)\n",
    "\n",
    "    coverages_adaptive.append(int(y_test in C_adapt))\n",
    "    sizes_adaptive.append(len(C_adapt))\n",
    "    alphas_adaptive.append(alpha_adapt)\n",
    "\n",
    "# results\n",
    "print(f\"Mean Alpha: {np.mean(alphas_adaptive):.4f}\")\n",
    "print(f\"Empirical Coverage: {np.mean(coverages_adaptive):.4f}\")\n",
    "print(f\"Expected Guarantee: {1 - np.mean(alphas_adaptive):.4f}\")\n",
    "print(f\"Average Set Size: {np.mean(sizes_adaptive):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tueplots import bundles\n",
    "import numpy as np\n",
    "\n",
    "# style setup\n",
    "plt.rcParams.update(bundles.icml2024())\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"axes.linewidth\": 2,\n",
    "})\n",
    "\n",
    "iterations = np.arange(1, len(lambda_history)+1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(iterations, mean_sizes_history, marker=\"o\", label=\"Mean Size\",linewidth=4,markersize=10)\n",
    "\n",
    "xshift = [0.1,0.15,0.1,0.05,-0.4] # adjust for visualization purpose\n",
    "yshift = [-0.1,-0.7,-0.6,0.3,-0.9]\n",
    "for i, (it, mean_size, lam) in enumerate(zip(iterations, mean_sizes_history, lambda_history)):\n",
    "    plt.text(it + xshift[i], mean_size+yshift[i], f\"$\\lambda$={lam:.0f}\", fontsize=16, color=\"black\")\n",
    "\n",
    "# target line\n",
    "plt.axhline(y=M, color=\"red\", linestyle=\"--\", label=f\"Target M={M}\",linewidth=3)\n",
    "\n",
    "# tolerance band (transparent red)\n",
    "plt.fill_between(\n",
    "    np.array([iterations[0] - 1, iterations[-1] + 1]), # iterations,\n",
    "    M - epsilon,\n",
    "    M + epsilon,\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    "    label=f\"Tolerance ±{epsilon}\"\n",
    ")\n",
    "\n",
    "plt.ylim(0, 10.5)\n",
    "plt.xlim(0.8,iterations[-1]+0.2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Size\")\n",
    "plt.xticks(iterations)\n",
    "plt.legend(frameon=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/algo_lambda.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
